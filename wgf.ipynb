{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4dec8e",
   "metadata": {},
   "source": [
    "# Numerical Wasserstein Gradient Flow via Sinkhorn's Algorithm\n",
    "Throughout the notebook, let us adapt the following setting. For $n \\in \\mathbb{N}$, define $\\Omega \\subseteq \\mathbb{R}^n$ be a compact subset of $n$ dimensional Euclidean space, and $\\mathcal{P}(\\Omega)$ be a space of Borel probability measures on $\\Omega$. The 2-Wasserstein space is defined as a metric space of probability measures endowed with what so called a 2-Wasserstein metric $W_2: \\mathcal{P}(\\Omega) \\times \\mathcal{P}(\\Omega) \\to [0,\\infty]$, defined as, $$W_2(\\mu,\\nu) = \\left( \\inf_{\\gamma \\in \\Pi(\\mu,\\nu)} \\int_{\\Omega^2} \\vert x-y\\vert^2 d\\gamma(x,y) \\right)^{1/2}$$ where $\\Pi(\\mu,\\nu) \\coloneqq \\{\\gamma \\in \\mathcal{P}(\\Omega^2) : \\pi_{1}\\sharp\\gamma = \\mu, \\pi_{2}\\sharp\\gamma = \\nu\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jko_lab import * # solvers\n",
    "from src.utils import * # graphing utilities\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import annotations\n",
    "Array = jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fe2b3",
   "metadata": {},
   "source": [
    "## Entropy Flow\n",
    "Shannon entropy $\\mathcal{F}[\\rho]: \\mathcal{P}(\\Omega) \\to \\mathbb{R}$ is a functional defined by\n",
    "$$\n",
    "F[\\rho] = \\int_{\\Omega} \\rho (\\log \\rho - 1) \\: d\\rho.\n",
    "$$\n",
    "Then, by taking $\\mathcal{F}[\\rho]$ as an external potential for the following entropic JKO scheme\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\mathcal{P}(\\Omega)}{\\arg\\min} \\left\\{ \\int_{\\Omega} \\rho (\\log \\rho - 1) d\\rho + \\frac{1}{2\\eta} W_{2,\\epsilon}^2(\\rho,\\rho^k) \\right\\},\n",
    "$$\n",
    "as JKO step size $\\eta \\to 0$, corresponding Wasserstein gradient flow $(\\rho^{k})_{k\\in \\mathbb{N}}$ approximates the heat equation\n",
    "$$\n",
    "\\partial_t \\rho_t = \\nabla \\cdot (\\nabla \\rho)\n",
    "$$\n",
    "with $\\rho_0 = \\rho^0 \\in \\mathcal{P}(\\Omega)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8da01b",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Upon spatial discretization in $n$ grid points, we get the explicit formulation of discrete Shannon entropy as\n",
    "$$\\mathcal{F}[\\rho] = \\sum_{i=1}^{n} \\rho_i (\\log \\rho_i - 1)$$\n",
    "with $\\rho \\in \\Sigma_n$. Since this functional is separable, its first variation, i.e. derivative, is given by\n",
    "$$\\frac{\\delta F}{\\delta \\rho} = \\sum_{i=1}^{n} \\log \\rho_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_shannon_entropy(rho: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Shannon entropy of a probability measure rho\n",
    "    \"\"\"\n",
    "    rho_safe = jnp.clip(rho, 1e-12, None) # prevent log(0)\n",
    "    return jnp.sum(rho_safe * jnp.log(rho_safe) - rho_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a48434",
   "metadata": {},
   "source": [
    "In this example, let us flow for a total $T = \\eta \\times n = 50$ where $\\eta$ and $n$ are JKO step size and the number of JKO steps, respectively. Let us start from a bimodal initial distribution $\\rho^0 = 0.5\\mathcal{N}(0.25,0.03)+0.5\\mathcal{N}(0.75,0.04)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237313c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100             # spatial discretization                \n",
    "T_TARGET = 50       # total time of the flow\n",
    "\n",
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "X, Y = jnp.meshgrid(x, x, indexing=\"ij\")\n",
    "C = (X - Y) ** 2\n",
    "\n",
    "# initial measure\n",
    "rho0 = 0.5 * jax.scipy.stats.norm.pdf(x, 0.25, 0.03) + 0.5 * jax.scipy.stats.norm.pdf(x,0.75,0.04)\n",
    "rho0 = jnp.clip(rho0, 1e-12, None); rho0 = rho0 / rho0.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f8b6b",
   "metadata": {},
   "source": [
    "Due to the nature of heat equation, as $k \\to \\infty$, sequence of probability measures $\\rho^k$ generated from the scheme should converge to an uniform distribution $b \\in \\mathcal{P}(\\Omega)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target measure is uniform\n",
    "b = jnp.ones_like(x)/n\n",
    "b = b / b.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47200a82",
   "metadata": {},
   "source": [
    "The numerical stability of the scheme is sharply determined by the choice of $\\eta$ and $\\epsilon$. After running a hyperparameter optimization on $(\\eta, \\epsilon)$, it found that $\\eta = 10^{-2}$ and $\\epsilon = 15$ are optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308bd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3                              # JKO step size\n",
    "epsilon = 1e-1                          # regularization parameter of inner Sinkhorn iterations\n",
    "sinkhorn_max_iters = 2000               # maximum number of inner Sinkhorn iterations\n",
    "jko_tol = 1e-8                          # tolerance for inner Sinkhorn approximation\n",
    "jko_lr = 0.01                           # learning rate of outer SGD iterations\n",
    "jko_inner_steps = 10                    # number of outer SGD iterations\n",
    "num_jko_steps = int(T_TARGET / eta)     # automatically determined by eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23298a1d",
   "metadata": {},
   "source": [
    "### Convergence and Diagnostics\n",
    "For each $k\\in 0$, run the following JKO scheme with Sinkhorn and SGD approximation\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\Sigma_n}{\\arg\\min} \\left\\{ \\sum_{i=1}^{n} \\rho_i(\\log\\rho_i-1) + \\frac{1}{2\\eta}\n",
    "\\sup_{f,g \\in \\mathbb{R}^n} \\left[ \\sum_{i=1}^{n} f_i \\rho_i + \\sum_{i=1}^{n} g_i \\rho^k_i - \\epsilon \\sum_{i,j} e^{f_i + g_i - c_{ij}} \\right]\\right\\}\n",
    "$$\n",
    "with parameters that are chosen from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jko_flow = SinkhornJKO(\n",
    "    C=C,                                # Euclidean distance matrix\n",
    "    rho0=rho0,                          # starting marginal\n",
    "    eta=eta,                            # JKO step size\n",
    "    epsilon=epsilon,                    # regularization parameter\n",
    "    F_func=F_shannon_entropy,           # external potential\n",
    "    sinkhorn_iters=sinkhorn_max_iters,  # set to 2,000\n",
    "    inner_steps=jko_inner_steps,        # set to 10\n",
    "    tol=jko_tol,                        # set to 1e-9\n",
    "    learning_rate=jko_lr,               # set to 0.01\n",
    "    optimizer_name='sgd'                # may choose adam, sgd, or etc.\n",
    ")\n",
    "\n",
    "rhos, diag = jko_flow.compute_flow(num_steps=num_jko_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04288be0",
   "metadata": {},
   "source": [
    "Let us verify the convergence of the algorithm. As the heat equation must flow to maximize the entropy $$E[\\rho] = -\\int \\rho \\log \\rho d\\rho,$$ we should witness an increment in entropy and decrement in distance to uniform target marginal. Measure $\\vert\\vert \\rho^k - b \\vert\\vert_{2}$ and $E[\\rho^k]$ for each $k \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(r):\n",
    "    r = jnp.clip(r, 1e-12, None)\n",
    "    return -jnp.sum(r * jnp.log(r))\n",
    "\n",
    "print(\"Computing L2 errors of measures...\")\n",
    "dists = jnp.linalg.norm(rhos - b[None, :], axis=1)\n",
    "print(\"Computing Entropy of measures...\")\n",
    "Hs = jnp.array([entropy(r) for r in rhos])\n",
    "\n",
    "# Check for convergence\n",
    "uniform_H = entropy(b)\n",
    "print(f\"\\nTarget Uniform Entropy: {float(uniform_H):.6f}\")\n",
    "\n",
    "print(\"[Flow-Entropy] Entropy should increase, distance to uniform should decrease:\")\n",
    "k_steps_to_print = [i for i in [0, 10, 50, 100, 500, 1000, 2500, num_jko_steps-1] if i < num_jko_steps]\n",
    "\n",
    "for k in k_steps_to_print:\n",
    "    print(f\" k={k:3d}: H={float(Hs[k]):.6f}, ||rho-b||={float(dists[k]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33187e",
   "metadata": {},
   "source": [
    "The following diagnostic illustrates how warm starting JKO steps with Kantorovich potentials from previous step benefits the convergence speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_jko_steps, num_jko_steps+1)\n",
    "sinkhorn_iters_per_jko = np.insert(np.asarray(diag['sinkhorn_iters_per_jko_step'].sum(axis=1)),0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f\"JKO Flow Diagnostics (Shannon Entropy, $\\\\eta={eta}$, Total JKO steps={num_jko_steps})\", fontsize=14)\n",
    "\n",
    "# Plot 1: Sinkhorn Iterations\n",
    "axes[0].plot(x, sinkhorn_iters_per_jko, linestyle='-', color='tab:blue', linewidth=1)\n",
    "axes[0].set_title('Sinkhorn Iters per JKO Step', fontsize=12)\n",
    "axes[0].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[0].set_ylabel('Total Sinkhorn Iterations (log scale)', fontsize=11)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "axes[0].tick_params(axis='y', which='minor', labelsize=8)\n",
    "\n",
    "# Plot 2: Entropy\n",
    "axes[1].plot(x, Hs, linestyle='-', color='tab:red', linewidth=1)\n",
    "axes[1].axhline(y=float(uniform_H), color='k', linestyle='--', label=r'$H(\\mathbf{b}) \\approx 4.605$')\n",
    "axes[1].set_title('Entropy $H(\\\\rho^k)$', fontsize=12)\n",
    "axes[1].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[1].set_ylabel('Entropy (nats)', fontsize=11)\n",
    "axes[1].set_ylim(bottom=Hs.min() - 0.1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 3: Distance\n",
    "axes[2].plot(x, dists, linestyle='-', color='tab:green', linewidth=1)\n",
    "axes[2].set_title(r'Distance to Uniform $||\\rho^k - \\mathbf{b}||_2$', fontsize=12)\n",
    "axes[2].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[2].set_ylabel(r'Euclidean Distance', fontsize=11)\n",
    "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb55f6a",
   "metadata": {},
   "source": [
    "The following cell describes the flow in animated histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "html = animate_hist_flow(  \n",
    "    mu_list=rhos, x=np.asarray(x),\n",
    "    target=b,\n",
    "    interval=50,\n",
    "    title=\"JKO Flow (Shannon Entropy)\"\n",
    ")\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad22d6e",
   "metadata": {},
   "source": [
    "## Porous Media Equation\n",
    "Let $\\mathcal{F}[\\rho]$ be a functional defined by\n",
    "$$\\mathcal{F}[\\rho]=\\frac{1}{m-1}\\int_{\\Omega}\\rho^m d\\rho,$$\n",
    "for $m\\geq 1$. Then, by taking $\\mathcal{F}[\\rho]$ as an external potential for the following entropic JKO scheme\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho\\in\\mathcal{P}(\\Omega)}{\\arg\\min}\\left\\{ \\frac{1}{m-1}\\int_{\\Omega}\\rho^m d\\rho + \\frac{W_2^2(\\rho,\\rho^k)}{2\\eta} \\right\\}\n",
    "$$\n",
    "as JKO step size $\\eta \\to 0$, corresponding Wasserstein gradient flow $(\\rho^k)_{k\\in\\mathbb{N}}$ approximates the porous media equation\n",
    "$$\\partial_t \\rho_t = \\nabla^2 \\mathcal{F}(\\rho) = \\nabla \\cdot (\\nabla \\mathcal{F}(\\rho))$$\n",
    "with $\\rho_0 = \\rho^0 \\in \\mathcal{P}(\\Omega)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42392bd8",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Upon spatial discretization in $n$ grid points, we get the explicit formulation of discrete porous media entropy as\n",
    "$$F[\\rho] = \\frac{1}{m-1}\\sum_{i=1}^{n} \\rho_i^m$$\n",
    "with $m > 1$. As a separable functional on Euclidean space, its first variation, which is equivalent to the derivative in this case, is going to be\n",
    "$$\n",
    "\\frac{\\delta F}{\\delta\\rho} = \\frac{m}{m-1} \\sum_{i=1}^{n} \\rho_i^{m-1}\n",
    "$$\n",
    "via pointwise differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_F_porous_media(rho, m=2):\n",
    "    \"\"\"\n",
    "    Gradient of porous media entropy of a probability measure rho with (m > 1)\n",
    "    \"\"\"\n",
    "    return jnp.sum(jnp.power(rho, m-1).clip(1e-100, None)) * m/(m-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31711b2",
   "metadata": {},
   "source": [
    "In this example, let us flow for a total $T = \\eta \\times n = 50$ where $\\eta$ and $n$ are JKO step size and the number of JKO steps, respectively. Let us start from a bimodal initial distribution $\\rho^0 = 0.6\\mathcal{N}(0.3,0.04) + 0.4\\mathcal{N}(0.75,0.05)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce13925",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300                 # spatial discretization\n",
    "T_TARGET = 5            # total time of the flow\n",
    "m = 1.1                 # try 1.5 or 3.0 as well\n",
    "\n",
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "X, Y = jnp.meshgrid(x, x, indexing=\"ij\")\n",
    "C = (X - Y) ** 2\n",
    "\n",
    "# initial measure\n",
    "rho0 = 0.6 * jax.scipy.stats.norm.pdf(x, 0.3, 0.04) + 0.4 * jax.scipy.stats.norm.pdf(x, 0.75, 0.05)\n",
    "rho0 = jnp.clip(rho0, 1e-18, None); rho0 = rho0 / rho0.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e50ee",
   "metadata": {},
   "source": [
    "Due to the nature of porous media equation, as $k \\to infty$, a sequence of probability measures $\\rho^k$ generated from the scheme should converge to an uniform distribution $b \\in \\mathcal{P}(\\Omega)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target measure is uniform\n",
    "b = jnp.ones_like(x) / n\n",
    "b = b / b.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e64ac",
   "metadata": {},
   "source": [
    "The numerical stability of the scheme is sharply determined by the choice of $\\eta$ and $\\epsilon$. After running a hyperparameter optimization on $(\\eta, \\epsilon)$, it found that $\\eta = 10^{-2}$ and $\\epsilon = 15$ are optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318007c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3                              # JKO step size\n",
    "epsilon = 5e-2                          # regularization parameter of inner Sinkhorn iterations\n",
    "sinkhorn_max_iters = 15000              # maximum number of inner Sinkhorn iterations\n",
    "jko_tol = 1e-9                          # tolerance for inner Sinkhorn approximation\n",
    "jko_lr = 0.01                           # learning rate of outer SGD iterations\n",
    "jko_inner_steps = 10                    # number of outer SGD iterations\n",
    "num_jko_steps = 200                     # automatically determined by eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c25f7",
   "metadata": {},
   "source": [
    "### Convergence and Diagnostics\n",
    "For each $k > 0$, run the following JKO scheme with Sinkhorn and SGD approximation\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\Sigma_n}{\\arg\\min} \\left\\{\\sum_{i=1}^{n} \\rho_i^m + \\frac{1}{2\\eta}\n",
    "\\sup_{f,g \\in \\mathbb{R}^n} \\left[ \\sum_{i=1}^{n} f_i \\rho_i + \\sum_{i=1}^{n} g_i \\rho^k_i - \\epsilon \\sum_{i,j} e^{f_i + g_i - c_{ij}} \\right]\\right\\}\n",
    "$$\n",
    "with parameters that are chosen from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02504ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jko_flow = SinkhornJKO(\n",
    "    C=C,                                # Euclidean distance matrix\n",
    "    rho0=rho0,                          # starting marginal\n",
    "    eta=eta,                            # JKO step size\n",
    "    epsilon=epsilon,                    # regularization parameter\n",
    "    grad_F_func=grad_F_porous_media,    # external potential\n",
    "    sinkhorn_iters=sinkhorn_max_iters,  # set to 2,000\n",
    "    inner_steps=jko_inner_steps,        # set to 10\n",
    "    tol=jko_tol,                        # set to 1e-9\n",
    "    learning_rate=jko_lr,               # set to 0.01\n",
    "    optimizer_name='adam'               # may choose adam, sgd, or etc.\n",
    ")\n",
    "\n",
    "rhos, diag = jko_flow.compute_flow(num_steps=num_jko_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45580f",
   "metadata": {},
   "source": [
    "Let us verify the convergence of the algorithm. We should witness decrement in both distance to uniform target marginal $b$ and peak of the distribution $\\rho^k$. Measure $\\vert\\vert \\rho^k - b\\vert\\vert_2$ and $\\max[\\rho^k]$ for each $k \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203eef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_to_uniform(r):\n",
    "    return float(jnp.linalg.norm(r - b))\n",
    "\n",
    "def rmax(r):\n",
    "    return float(jnp.max(r))\n",
    "\n",
    "print(\"[Flow-PorousMedia] toward uniform (expect ||rho-b|| ↓, max(rho) ↓):\")\n",
    "for k in [0, 1, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500]:\n",
    "    r = rhos[k]\n",
    "    print(f\"  k={k:2d}: ||rho-b||={l2_to_uniform(r):.6f}, max(rho)={rmax(r):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336cdd1",
   "metadata": {},
   "source": [
    "The following diagnostic illustrates how warm starting JKO steps with Kantorovich potentials from previous step benefits the convergence speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d89432",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_jko_steps, num_jko_steps+1)\n",
    "sinkhorn_iters_per_jko = np.insert(np.asarray(diag['sinkhorn_iters_per_jko_step'].sum(axis=1)),0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f\"JKO Flow Diagnostics (Shannon Entropy, $\\\\eta={eta}$, Total JKO steps={num_jko_steps})\", fontsize=14)\n",
    "\n",
    "# Plot 1: Sinkhorn Iterations\n",
    "axes[0].plot(x, sinkhorn_iters_per_jko, linestyle='-', color='tab:blue', linewidth=1)\n",
    "axes[0].set_title('Sinkhorn Iters per JKO Step', fontsize=12)\n",
    "axes[0].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[0].set_ylabel('Total Sinkhorn Iterations (log scale)', fontsize=11)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "axes[0].tick_params(axis='y', which='minor', labelsize=8)\n",
    "\n",
    "# Plot 2: Entropy\n",
    "axes[1].plot(x, Hs, linestyle='-', color='tab:red', linewidth=1)\n",
    "axes[1].axhline(y=float(uniform_H), color='k', linestyle='--', label=r'$H(\\mathbf{b}) \\approx 4.605$')\n",
    "axes[1].set_title('Entropy $H(\\\\rho^k)$', fontsize=12)\n",
    "axes[1].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[1].set_ylabel('Entropy (nats)', fontsize=11)\n",
    "axes[1].set_ylim(bottom=Hs.min() - 0.1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 3: Distance\n",
    "axes[2].plot(x, dists, linestyle='-', color='tab:green', linewidth=1)\n",
    "axes[2].set_title(r'Distance to Uniform $||\\rho^k - \\mathbf{b}||_2$', fontsize=12)\n",
    "axes[2].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[2].set_ylabel(r'Euclidean Distance', fontsize=11)\n",
    "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3025a",
   "metadata": {},
   "source": [
    "The following cell describes the flow in animated histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "html = animate_hist_flow(  \n",
    "    mu_list=rhos, x=np.asarray(x),\n",
    "    target=b,\n",
    "    interval=50,\n",
    "    title=\"JKO Flow (porous media entropy)\"\n",
    ")\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663fe481",
   "metadata": {},
   "source": [
    "## $\\chi$-square Divergence Flow\n",
    "$\\chi$-squared divergence $\\mathcal{F}[\\rho]: \\mathcal{P}(\\Omega) \\to \\mathbb{R}$ is a functional defined by\n",
    "$$\\mathcal{F}[\\rho] = \\int_{\\Omega} \\left(\\frac{d\\rho}{d\\nu}-1\\right)^2 d\\nu$$\n",
    "where $\\rho$ is absolutely continuous with respect to a reference measure $\\nu$. By Radon-Nikodym theorem, $\\rho$ admits a density, and its Radon-Nikodym derivative is denoted by $\\frac{d\\rho}{d\\nu}$. By taking $\\mathcal{F}[\\rho]$ as an external potential for the following entropic JKO scheme,\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\mathcal{P}(\\Omega)}{\\arg\\min} \\left\\{ \\int_{\\Omega} \\left(\\frac{d\\rho}{d\\nu} - 1\\right)^2 d\\nu + \\frac{1}{2\\eta} W_{2,\\epsilon}^2(\\rho,\\rho^k) \\right\\},\n",
    "$$\n",
    "as JKO step size $\\eta \\to 0$, corresponding Wasserstein gradient flow $(\\rho^k)_{k\\in\\mathbb{N}}$ is called a $\\chi$-square divergence flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63687d",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Upon spatial discretization in $n$ grid points, we get the explicit formulation of discrete $\\chi^2$ divergence as\n",
    "$$F[\\rho] = \\sum_{i=1}^{n} \\frac{\\rho_i^2}{\\nu_i}$$\n",
    "with a reference probability vector $\\nu \\in \\Sigma_n$.\n",
    "As a separable functional on Euclidean space, its first variation which coincides the derivative in this case, is going to be\n",
    "$$\\frac{\\delta F}{\\delta\\rho} = \\sum_{i=1}^{n} \\frac{2\\rho_i}{\\nu_i}$$\n",
    "via pointwise differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56124bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square divergence\n",
    "def grad_F_chi_square_div(\n",
    "        rho: jnp.ndarray,\n",
    "        nu: jnp.ndarray\n",
    "    ) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Gradient of chi-square divergence of probability measure rho given a reference measure b\n",
    "    \"\"\"\n",
    "    nu = jnp.clip(b, 1e-8, None)\n",
    "    return jnp.sum(jnp.divide(2*rho, nu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d6b29",
   "metadata": {},
   "source": [
    "In this example, let us flow for a total $T = \\eta \\times n = 50$ where $\\eta$ and $n$ are JKO step size and the number of JKO steps, respectively. Let us start from a Gaussian initial distribution $\\rho^0 = \\mathcal{N}(0.1,0.04)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 512\n",
    "T_TARGET = 5\n",
    "\n",
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "X, Y = jnp.meshgrid(x, x, indexing=\"ij\")\n",
    "C = (X - Y) ** 2\n",
    "\n",
    "# initial measure\n",
    "rho0 = jax.scipy.stats.norm.pdf(x, 0.1, 0.04)\n",
    "rho0 = jnp.clip(rho0, 1e-18, None); rho0 = rho0 / rho0.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87b4bb",
   "metadata": {},
   "source": [
    "Lt us target to have a converging measure $b \\in \\mathcal{P}(\\Omega)$ which is a Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target marginal b\n",
    "b = jax.scipy.stats.cauchy.pdf(x, 0.7, 0.2)\n",
    "b = b / b.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276ea4f",
   "metadata": {},
   "source": [
    "The numerical stability of the scheme is sharply determined by the choice of $\\eta$ and $\\epsilon$. After running a hyperparameter optimization on $(\\eta, \\epsilon)$, it found that $\\eta = 10^{-2}$ and $\\epsilon = 15$ are optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3                              # JKO step size\n",
    "epsilon = 5e-2                          # regularization parameter of inner Sinkhorn iterations\n",
    "sinkhorn_max_iters = 15000              # maximum number of inner Sinkhorn iterations\n",
    "jko_tol = 1e-9                          # tolerance for inner Sinkhorn approximation\n",
    "jko_lr = 0.01                           # learning rate of outer SGD iterations\n",
    "jko_inner_steps = 10                    # number of outer SGD iterations\n",
    "num_jko_steps = 500                     # automatically determined by eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e229ded",
   "metadata": {},
   "source": [
    "### Convergence and Diagnostics\n",
    "For each $k\\in 0$, run the following JKO scheme with Sinkhorn and SGD approximation\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\Sigma_n}{\\arg\\min} \\left\\{ \\sum_{i=1}^{n} \\rho_i^m + \\frac{1}{2\\eta}\n",
    "\\sup_{f,g \\in \\mathbb{R}^n} \\left[ \\sum_{i=1}^{n} f_i \\rho_i + \\sum_{i=1}^{n} g_i \\rho^k_i - \\epsilon \\sum_{i,j} e^{f_i + g_i - c_{ij}} \\right]\\right\\}\n",
    "$$\n",
    "with parameters that are chosen from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "jko_flow = SinkhornJKO(\n",
    "    C=C,                                # Euclidean distance matrix\n",
    "    rho0=rho0,                          # starting marginal\n",
    "    eta=eta,                            # JKO step size\n",
    "    epsilon=epsilon,                    # regularization parameter\n",
    "    grad_F_func=grad_F_chi_square_div,  # external potential\n",
    "    sinkhorn_iters=sinkhorn_max_iters,  # set to 2,000\n",
    "    inner_steps=jko_inner_steps,        # set to 10\n",
    "    tol=jko_tol,                        # set to 1e-9\n",
    "    learning_rate=jko_lr,               # set to 0.01\n",
    "    optimizer_name='adam'               # may choose adam, sgd, or etc.\n",
    ")\n",
    "\n",
    "rhos, diag = jko_flow.compute_flow(num_steps=num_jko_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3535ef4",
   "metadata": {},
   "source": [
    "Let us verify the convergence of the algorithm. We should witness a decrement in distance to uniform target marginal. Measure $\\vert\\vert \\rho^k - b \\vert\\vert_{2}$ for each $k \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(r):\n",
    "    return float(jnp.linalg.norm(r-b))\n",
    "\n",
    "print(\"[Flow-chi2] Expect ||rho-b|| to decrease\")\n",
    "for k in [0, 1, 5, 10, 20, 40, 60, 80, 100]:\n",
    "    print(f\"  k={k:2d}: ||rho-b||={dist(rhos[k]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5e1f6",
   "metadata": {},
   "source": [
    "The following diagnostic illustrates how warm starting JKO steps with Kantorovich potentials from previous step benefits the convergence speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f502bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_jko_steps, num_jko_steps+1)\n",
    "sinkhorn_iters_per_jko = np.insert(np.asarray(diag['sinkhorn_iters_per_jko_step'].sum(axis=1)),0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f\"JKO Flow Diagnostics (Shannon Entropy, $\\\\eta={eta}$, Total JKO steps={num_jko_steps})\", fontsize=14)\n",
    "\n",
    "# Plot 1: Sinkhorn Iterations\n",
    "axes[0].plot(x, sinkhorn_iters_per_jko, linestyle='-', color='tab:blue', linewidth=1)\n",
    "axes[0].set_title('Sinkhorn Iters per JKO Step', fontsize=12)\n",
    "axes[0].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[0].set_ylabel('Total Sinkhorn Iterations (log scale)', fontsize=11)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "axes[0].tick_params(axis='y', which='minor', labelsize=8)\n",
    "\n",
    "# Plot 2: Entropy\n",
    "axes[1].plot(x, Hs, linestyle='-', color='tab:red', linewidth=1)\n",
    "axes[1].axhline(y=float(uniform_H), color='k', linestyle='--', label=r'$H(\\mathbf{b}) \\approx 4.605$')\n",
    "axes[1].set_title('Entropy $H(\\\\rho^k)$', fontsize=12)\n",
    "axes[1].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[1].set_ylabel('Entropy (nats)', fontsize=11)\n",
    "axes[1].set_ylim(bottom=Hs.min() - 0.1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 3: Distance\n",
    "axes[2].plot(x, dists, linestyle='-', color='tab:green', linewidth=1)\n",
    "axes[2].set_title(r'Distance to Uniform $||\\rho^k - \\mathbf{b}||_2$', fontsize=12)\n",
    "axes[2].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[2].set_ylabel(r'Euclidean Distance', fontsize=11)\n",
    "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccebc1",
   "metadata": {},
   "source": [
    "The following cell describes the flow in animated histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "html = animate_hist_flow(  \n",
    "    mu_list=rhos, x=np.asarray(x),\n",
    "    target=b,\n",
    "    interval=50,\n",
    "    title=\"JKO Flow (chi-square divergence)\"\n",
    ")\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045bf04",
   "metadata": {},
   "source": [
    "## Porous Media Divergence Flow\n",
    "Let $V$ be a lower bounded, lower semicontinuous, and convex function. Porous media divergence $\\mathcal{F}[\\rho] : \\mathcal{P}(\\Omega) \\to \\mathbb{R}$ is a functional defined by\n",
    "$$\n",
    "\\mathcal{F}[\\rho] = \\int_{\\Omega} V d\\rho + \\frac{1}{m-1}\\int_{\\Omega} \\rho^m d\\rho\n",
    "$$ for some $m \\geq 1$. If $V=0$ is identically zero, then porous media divergence is equivalent to the porous media entropy. Then, by taking $\\mathcal{F}[\\rho]$ as an external potential for the following entropic JKO scheme\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\mathcal{P}(\\Omega)}{\\arg\\min} \\left\\{ \\int_{\\Omega} \\rho (\\log \\rho - 1) d\\rho + \\frac{1}{2\\eta} W_{2,\\epsilon}^2(\\rho,\\rho^k) \\right\\},\n",
    "$$\n",
    "as JKO step size $\\eta \\to 0$, corresponding Wasserstein gradient flow $(\\rho^k)_{k\\in\\mathbb{N}}$ is called *porous media divergence flow* with $\\rho_0 = \\rho^0 \\in \\mathcal{P}(\\Omega)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9853ed0",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Upon spatial discretization in $n$ grid points and the choice of $V$ to be $V_i = -\\log b_i$ for some reference measure $b \\in \\Sigma^n$, we get the explicit formulation of discrete porous media divergence as\n",
    "$$\n",
    "F[\\rho] = \\sum_{i=1}^{n} -\\rho_i\\log b_i + \\frac{1}{m-1}\\sum_{i=1}^{n} \\rho^m_{i}\n",
    "$$ for a given $m \\geq 1$. As a separable functional on Euclidean space, its first variation, which is equivalent to the derivative in this case, is going to be\n",
    "$$\n",
    "\\frac{\\delta F}{\\delta \\rho} = - \\sum_{i=1}^{n} \\log b_i + \\frac{m}{m-1} \\sum_{i=1}^{n} \\rho^{m-1}_i\n",
    "$$\n",
    "via pointwise differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_F_porous_divergence(rho, V, m):\n",
    "    \"\"\"\n",
    "    Gradient of porous media divergence of rho given m >= 1 and potential vector V\n",
    "    \"\"\"\n",
    "    return jnp.sum(V)+(jnp.sum(jnp.power(rho,m-1)))*m/(m-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2131d",
   "metadata": {},
   "source": [
    "In this example, let us flow for a total $T=\\eta \\times n = 50$ where $\\eta$ and $n$ are JKO step size and the number of JKO steps, respectively. Let us start from a bimodal initial distribution $\\rho^0 = 0.7\\mathcal{N}(0.25,0.05)+0.3\\mathcal{N}(0.75,0.06)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128                 # spatial discretization\n",
    "T_TARGET = 50           # total time of the flow\n",
    "m = 1.5                 # 1.2, 1.5, 2.0 (all displacement-convex for m>=1)\n",
    "\n",
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "X, Y = jnp.meshgrid(x, x, indexing=\"ij\")\n",
    "C = (X - Y) ** 2\n",
    "\n",
    "# initial measure\n",
    "rho0 = 0.7 * jax.scipy.stats.norm.pdf(x, 0.25, 0.05) + 0.3 * jax.scipy.stats.norm.pdf(x, 0.75, 0.06)\n",
    "rho0 = jnp.clip(rho0, 1e-18, None); rho0 = rho0 / rho0.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1436f",
   "metadata": {},
   "source": [
    "The sequence of probability measures $\\rho^k$ generated from the scheme should converge to a Gaussian distribution $\\mathcal{N}(0.6,0.07) \\in \\mathcal{P}(\\Omega)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target measure is gaussian\n",
    "mu, sigma = 0.6, 0.07\n",
    "b = jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "b = jnp.clip(b, 1e-18, None); b = b / b.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a6ef4",
   "metadata": {},
   "source": [
    "The numerical stability of the scheme is sharply determined by the choice of $\\eta$ and $\\epsilon$. After running a hyperparameter optimization on $(\\eta, \\epsilon)$, it found that $\\eta = 10^{-2}$ and $\\epsilon = 15$ are optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3                              # JKO step size\n",
    "epsilon = 5e-2                          # regularization parameter of inner Sinkhorn iterations\n",
    "sinkhorn_max_iters = 15000              # maximum number of inner Sinkhorn iterations\n",
    "jko_tol = 1e-9                          # tolerance for inner Sinkhorn approximation\n",
    "jko_lr = 0.01                           # learning rate of outer SGD iterations\n",
    "jko_inner_steps = 10                    # number of outer SGD iterations\n",
    "num_jko_steps = 500                     # automatically determined by eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538ba4d",
   "metadata": {},
   "source": [
    "### Convergence and Diagnostics\n",
    "For each $k\\in 0$, run the following JKO scheme with Sinkhorn and SGD approximation\n",
    "$$\n",
    "\\rho^{k+1} = \\underset{\\rho \\in \\Sigma_n}{\\arg\\min} \\left\\{ -\\sum_{i=1}^{n} \\rho_i \\log b_i + \\frac{1}{m-1}\\sum_{i=1}^{n} \\rho_i^{m} + \\frac{1}{2\\eta}\n",
    "\\sup_{f,g \\in \\mathbb{R}^n} \\left[ \\sum_{i=1}^{n} f_i \\rho_i + \\sum_{i=1}^{n} g_i \\rho^k_i - \\epsilon \\sum_{i,j} e^{f_i + g_i - c_{ij}} \\right]\\right\\}\n",
    "$$\n",
    "with parameters that are chosen from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jko_flow = SinkhornJKO(\n",
    "    C=C,                                    # Euclidean distance matrix\n",
    "    rho0=rho0,                              # starting marginal\n",
    "    eta=eta,                                # JKO step size\n",
    "    epsilon=epsilon,                        # regularization parameter\n",
    "    grad_F_func=grad_F_porous_divergence,   # external potential\n",
    "    sinkhorn_iters=sinkhorn_max_iters,      # set to 2,000\n",
    "    inner_steps=jko_inner_steps,            # set to 10\n",
    "    tol=jko_tol,                            # set to 1e-9\n",
    "    learning_rate=jko_lr,                   # set to 0.01\n",
    "    optimizer_name='adam'                   # may choose adam, sgd, or etc.\n",
    ")\n",
    "\n",
    "rhos, diag = jko_flow.compute_flow(num_steps=num_jko_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d985c",
   "metadata": {},
   "source": [
    "Let us verify the convergence of the algorithm. We should witness an increment of mass concentrated near $\\mu = 0.6$ of the target measure $b \\in \\mathcal{P}(\\Omega)$ and decrement in distance to Gaussian $b$. Measure $\\vert\\vert \\rho^k - b \\vert\\vert_2$ and $\\lambda([\\vert \\rho^k - \\mu \\vert \\leq 2\\sigma])$ where $\\lambda$ is the Lebesgue measure on $\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for convergence\n",
    "def dist_to_b(r):\n",
    "    return float(jnp.linalg.norm(r - b))\n",
    "\n",
    "def mass_near_center(r, rad=2*sigma):  # mass within ~2σ of mu\n",
    "    return float(jnp.sum(r * (jnp.abs(x - mu) <= rad)))\n",
    "\n",
    "print(\"[Flow-Porous+V(b)] toward Gaussian target b (expect ||rho-b|| ↓, mass near mu ↑):\")\n",
    "for k in [0, 1, 5, 10, 20, 40, 50, 100, 200, 300, 400, 500, 600]:\n",
    "    r = rhos[k]\n",
    "    print(f\"  k={k:2d}: ||rho-b||={dist_to_b(r):.6f}, mass(|x-mu|<=2σ)={mass_near_center(r):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdd827",
   "metadata": {},
   "source": [
    "The following diagnostic illustrates how warm starting JKO steps with Kantorovich potentials from previous step benefits the convergence speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_jko_steps, num_jko_steps+1)\n",
    "sinkhorn_iters_per_jko = np.insert(np.asarray(diag['sinkhorn_iters_per_jko_step'].sum(axis=1)),0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f\"JKO Flow Diagnostics (Shannon Entropy, $\\\\eta={eta}$, Total JKO steps={num_jko_steps})\", fontsize=14)\n",
    "\n",
    "# Plot 1: Sinkhorn Iterations\n",
    "axes[0].plot(x, sinkhorn_iters_per_jko, linestyle='-', color='tab:blue', linewidth=1)\n",
    "axes[0].set_title('Sinkhorn Iters per JKO Step', fontsize=12)\n",
    "axes[0].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[0].set_ylabel('Total Sinkhorn Iterations (log scale)', fontsize=11)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "axes[0].tick_params(axis='y', which='minor', labelsize=8)\n",
    "\n",
    "# Plot 2: Entropy\n",
    "axes[1].plot(x, Hs, linestyle='-', color='tab:red', linewidth=1)\n",
    "axes[1].axhline(y=float(uniform_H), color='k', linestyle='--', label=r'$H(\\mathbf{b}) \\approx 4.605$')\n",
    "axes[1].set_title('Entropy $H(\\\\rho^k)$', fontsize=12)\n",
    "axes[1].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[1].set_ylabel('Entropy (nats)', fontsize=11)\n",
    "axes[1].set_ylim(bottom=Hs.min() - 0.1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 3: Distance\n",
    "axes[2].plot(x, dists, linestyle='-', color='tab:green', linewidth=1)\n",
    "axes[2].set_title(r'Distance to Uniform $||\\rho^k - \\mathbf{b}||_2$', fontsize=12)\n",
    "axes[2].set_xlabel('JKO Step ($k$)', fontsize=11)\n",
    "axes[2].set_ylabel(r'Euclidean Distance', fontsize=11)\n",
    "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38152c76",
   "metadata": {},
   "source": [
    "The following cell describes the flow in animated histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(0.0, 1.0, n)\n",
    "html = animate_hist_flow(  \n",
    "    mu_list=rhos, x=np.asarray(x),\n",
    "    target=b,\n",
    "    interval=50,\n",
    "    title=\"JKO Flow (porous media divergence)\"\n",
    ")\n",
    "html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
